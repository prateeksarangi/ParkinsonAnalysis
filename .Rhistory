data <- read.csv("~/SummerInternship/data.json", header=FALSE)
View(data)
install.packages(c("ggplot2", "gridExtra", "kableExtra", "knitr", "RColorBrewer", "RWeka", "stringi", "tm", "wordcloud"))
install.packages(c("ggplot2", "gridExtra", "kableExtra", "knitr", "RColorBrewer", "RWeka", "stringi", "tm", "wordcloud"))
install.packages(c("ggplot2", "gridExtra", "kableExtra", "knitr", "RColorBrewer", "RWeka", "stringi", "tm", "wordcloud"))
install.packages(c("ggplot2", "gridExtra", "kableExtra", "knitr", "RColorBrewer", "RWeka", "stringi", "tm", "wordcloud"))
install.packages(c("ggplot2", "gridExtra", "kableExtra", "knitr", "RColorBrewer", "RWeka", "stringi", "tm", "wordcloud"))
library(knitr)
rm(list = ls(all.names = TRUE))
setwd("~/DataScienceCapstone")
# set knitr options
knitr::opts_chunk$set(echo = TRUE, fig.path = 'figures/')
# free up memory and display statistics on free memory
gc()
# disable scientific notation for numbers
options(scipen = 1)
# detect OS
switch(Sys.info()[['sysname']],
Windows = {os = "Microsoft Windows"},
Linux = {os = "Linux"},
Darwin = {os = "macOS"})
# knit hook to allow partial output from a code chunk
hook_output <- knit_hooks$get("output")
knit_hooks$set(output = function(x, options) {
lines <- options$output.lines
if (is.null(lines)) {
return(hook_output(x, options)) # pass to default hook
}
x <- unlist(strsplit(x, "\n"))
more <- "..."
if (length(lines) == 1) { # first n lines
if (length(x) > lines) {
# truncate the output, but add ....
x <- c(head(x, lines), more)
}
} else {
x <- c(more, x[lines], more)
}
# paste these lines together
x <- paste(c(x, ""), collapse = "\n")
hook_output(x, options)
})
# blogs
blogsFileName <- "final/en_US/en_US.blogs.txt"
con <- file(blogsFileName, open = "r")
blogs <- readLines(con, encoding = "UTF-8", skipNul = TRUE)
close(con)
# news
newsFileName <- "final/en_US/en_US.news.txt"
con <- file(newsFileName, open = "r")
news <- readLines(con, encoding = "UTF-8", skipNul = TRUE)
close(con)
# twitter
twitterFileName <- "final/en_US/en_US.twitter.txt"
con <- file(twitterFileName, open = "r")
twitter <- readLines(con, encoding = "UTF-8", skipNul = TRUE)
close(con)
rm(con)
library(stringi)
library(kableExtra)
install.packages("kableExtra")
library(knitr)
rm(list = ls(all.names = TRUE))
setwd("~/DataScienceCapstone")
# set knitr options
knitr::opts_chunk$set(echo = TRUE, fig.path = 'figures/')
# free up memory and display statistics on free memory
gc()
# disable scientific notation for numbers
options(scipen = 1)
# detect OS
switch(Sys.info()[['sysname']],
Windows = {os = "Microsoft Windows"},
Linux = {os = "Linux"},
Darwin = {os = "macOS"})
# knit hook to allow partial output from a code chunk
hook_output <- knit_hooks$get("output")
knit_hooks$set(output = function(x, options) {
lines <- options$output.lines
if (is.null(lines)) {
return(hook_output(x, options)) # pass to default hook
}
x <- unlist(strsplit(x, "\n"))
more <- "..."
if (length(lines) == 1) { # first n lines
if (length(x) > lines) {
# truncate the output, but add ....
x <- c(head(x, lines), more)
}
} else {
x <- c(more, x[lines], more)
}
# paste these lines together
x <- paste(c(x, ""), collapse = "\n")
hook_output(x, options)
})
# blogs
blogsFileName <- "final/en_US/en_US.blogs.txt"
con <- file(blogsFileName, open = "r")
blogs <- readLines(con, encoding = "UTF-8", skipNul = TRUE)
close(con)
# news
newsFileName <- "final/en_US/en_US.news.txt"
con <- file(newsFileName, open = "r")
news <- readLines(con, encoding = "UTF-8", skipNul = TRUE)
close(con)
# twitter
twitterFileName <- "final/en_US/en_US.twitter.txt"
con <- file(twitterFileName, open = "r")
twitter <- readLines(con, encoding = "UTF-8", skipNul = TRUE)
close(con)
rm(con)
library(stringi)
#library(kableExtra)
# assign sample size
sampleSize = 0.01
# file size
fileSizeMB <- round(file.info(c(blogsFileName,
newsFileName,
twitterFileName))$size / 1024 ^ 2)
# num lines per file
numLines <- sapply(list(blogs, news, twitter), length)
# num characters per file
numChars <- sapply(list(nchar(blogs), nchar(news), nchar(twitter)), sum)
# num words per file
numWords <- sapply(list(blogs, news, twitter), stri_stats_latex)[4,]
# words per line
wpl <- lapply(list(blogs, news, twitter), function(x) stri_count_words(x))
# words per line summary
wplSummary = sapply(list(blogs, news, twitter),
function(x) summary(stri_count_words(x))[c('Min.', 'Mean', 'Max.')])
rownames(wplSummary) = c('WPL.Min', 'WPL.Mean', 'WPL.Max')
summary <- data.frame(
File = c("en_US.blogs.txt", "en_US.news.txt", "en_US.twitter.txt"),
FileSize = paste(fileSizeMB, " MB"),
Lines = numLines,
Characters = numChars,
Words = numWords,
t(rbind(round(wplSummary)))
)
kable(summary,
row.names = FALSE,
align = c("l", rep("r", 7)),
caption = "") %>% kable_styling(position = "left")
library(kableExtra)
install.packages("kableExtra")
library(dplyr)
?dplry
?sum
library(sql)
?colSums
pd_speech_features <- read.csv("~/ParkinsonAnalysis/pd_speech_features.csv")
View(pd_speech_features)
setwd("~/ParkinsonAnalysis")
pd_speech_features <- read.csv("pd_speech_features.csv")
